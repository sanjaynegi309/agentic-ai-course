{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Hugging Face Integration\n",
    "\n",
    "## Tapping into the World's AI Marketplace\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/YOUR_REPOSITORY/blob/main/notebooks/06-HuggingFace-Integration.ipynb)\n",
    "\n",
    "So far, we've mostly used proprietary models like Gemini as our agent's 'brain'. But what if you need a specialized model, or want to ground your agent in a specific dataset? Welcome to **Hugging Face**, the GitHub of machine learning. In this chapter, you'll learn how to integrate its vast resources into your agentic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤— Why Integrate Hugging Face?\n",
    "\n",
    "The Hugging Face Hub is a massive repository of:\n",
    "\n",
    "- **Models:** Thousands of pre-trained models for various tasks (text generation, translation, summarization, etc.). You can find smaller, faster models or models with specializations.\n",
    "- **Datasets:** Thousands of datasets you can use to give your agents specialized knowledge.\n",
    "- **Tools:** A rich ecosystem of libraries like `transformers` and `datasets` that are the industry standard for working with ML models.\n",
    "\n",
    "By integrating Hugging Face, you can build more powerful, flexible, and customized agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install and Setup\n",
    "!pip install langchain transformers datasets huggingface_hub python-dotenv torch sentence-transformers\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "if not load_dotenv():\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
    "        os.environ['HUGGINGFACE_API_KEY'] = userdata.get('HUGGINGFACE_API_KEY')\n",
    "    except ImportError:\n",
    "        print(\"Could not load API keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Using a Hugging Face Model in LangChain\n",
    "\n",
    "Let's swap out Gemini for an open-source model from the Hub. We'll use the `HuggingFacePipeline` class in LangChain, which makes it easy to work with `transformers` pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load a Hugging Face Model as the LLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load a smaller, efficient model for demonstration\n",
    "hf_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-large\",\n",
    "    task=\"text2text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    ")\n",
    "\n",
    "template = \"Question: {question}\\nAnswer:\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=hf_llm)\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "print(chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Building a Hugging Face Dataset Tool\n",
    "\n",
    "This is where it gets really powerful. We can create a tool that allows an agent to search for information within a specific Hugging Face dataset. This gives the agent specialized knowledge that the base LLM doesn't have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Dataset Tool\n",
    "from langchain.tools import tool\n",
    "from datasets import load_dataset\n",
    "\n",
    "@tool\n",
    "def search_imdb_dataset(query: str) -> str:\n",
    "    \"\"\"Searches the IMDB dataset for movie reviews matching the query.\"\"\"\n",
    "    # Load the dataset (it will be downloaded and cached)\n",
    "    imdb = load_dataset(\"imdb\", split=\"train\")\n",
    "    # Perform a simple search\n",
    "    results = imdb.filter(lambda example: query.lower() in example['text'].lower())\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return f\"No reviews found for '{query}'.\"\n",
    "    \n",
    "    # Return the first few results\n",
    "    return \"\\n\".join([res['text'][:200] + '...' for res in results[:3]])\n",
    "\n",
    "print(search_imdb_dataset.name)\n",
    "print(search_imdb_dataset.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create an Agent with the Dataset Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Let's use Gemini as the 'brain' for its strong reasoning abilities\n",
    "reasoning_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "tools = [search_imdb_dataset]\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    reasoning_llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "agent.run(\"Find me a review of the movie 'Inception' that mentions the 'dream within a dream' concept.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Expanding the Possibilities\n",
    "\n",
    "You've just given your agent a new superpower: the ability to query a massive, specialized dataset. This same pattern can be used to create all sorts of powerful tools:\n",
    "\n",
    "- A **translation tool** that uses a `Helsinki-NLP` model.\n",
    "- A **summarization tool** that uses a `facebook/bart-large-cnn` model.\n",
    "- A **code generation tool** that uses a `CodeLlama` model.\n",
    "\n",
    "By combining a powerful reasoning model (like Gemini) with specialized tool-using models and datasets from Hugging Face, you can build incredibly capable and knowledgeable agents.\n",
    "\n",
    "In the next chapter, we'll go deeper into the **Gemini API** itself, exploring its native features like streaming and multimodal capabilities that go beyond the standard LangChain abstractions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
