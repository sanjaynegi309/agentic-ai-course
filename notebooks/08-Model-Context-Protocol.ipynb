{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: The Model Context Protocol (MCP)\n",
    "\n",
    "## A Universal Language for AI Agents\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/YOUR_REPOSITORY/blob/main/notebooks/08-Model-Context-Protocol.ipynb)\n",
    "\n",
    "Welcome to a more advanced, forward-looking chapter. We've seen how to build agents with different frameworks, but how do we make them talk to each other? In this chapter, we'll explore the **Model Context Protocol (MCP)**, a proposed standard for enabling interoperability between diverse AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—£ï¸ The Problem: The Agentic Tower of Babel\n",
    "\n",
    "Imagine you have a powerful research agent built with LangGraph and a creative writing agent built with CrewAI. You want them to collaborate. The problem is, they have completely different ways of managing their internal state, memory, and tools. They don't speak the same language.\n",
    "\n",
    "This lack of a common communication standard makes it incredibly difficult to build truly modular and interoperable multi-agent systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ The Solution: A Shared 'Whiteboard'\n",
    "\n",
    "The **Model Context Protocol (MCP)** proposes a simple but powerful solution: a central **Context Server**. Think of it as a shared digital whiteboard.\n",
    "\n",
    "- **Agents** can write information to the whiteboard.\n",
    "- **Agents** can read information from the whiteboard.\n",
    "\n",
    "Instead of talking to each other directly, agents communicate *through* this shared context. The protocol defines a standard way to structure this information (a **schema**), ensuring that all agents can understand it. This enables agents built with any framework to collaborate seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On: Building a Mini MCP Server\n",
    "\n",
    "Since MCP is still an emerging concept, there aren't many off-the-shelf libraries. So, we'll build our own simplified version from scratch using `FastAPI` to understand the core principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install and Setup\n",
    "!pip install fastapi uvicorn requests nest_asyncio python-dotenv\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # Allows Uvicorn to run in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: The MCP Server Code\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, Any\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# This dictionary is our simple, in-memory context store\n",
    "shared_context: Dict[str, Any] = {}\n",
    "\n",
    "class ContextUpdate(BaseModel):\n",
    "    key: str\n",
    "    value: Any\n",
    "\n",
    "@app.get(\"/context\")\n",
    "def get_context():\n",
    "    \"\"\"Reads the entire shared context.\"\"\"\n",
    "    return shared_context\n",
    "\n",
    "@app.post(\"/context\")\n",
    "def update_context(update: ContextUpdate):\n",
    "    \"\"\"Updates a specific key in the shared context.\"\"\"\n",
    "    shared_context[update.key] = update.value\n",
    "    return {\"status\": \"success\", \"updated_key\": update.key}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run the Server in the Background\n",
    "import uvicorn\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Run the server in a separate thread so it doesn't block the notebook\n",
    "server_thread = threading.Thread(target=run_server)\n",
    "server_thread.start()\n",
    "time.sleep(2) # Give the server a moment to start\n",
    "\n",
    "print(\"MCP Server is running in the background on port 8000.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating MCP-Aware \"Agents\"\n",
    "\n",
    "Now that our server is running, let's create two simple functions that act as our agents. They will communicate with the server using `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the Agents\n",
    "import requests\n",
    "import json\n",
    "\n",
    "SERVER_URL = \"http://localhost:8000\"\n",
    "\n",
    "def researcher_agent():\n",
    "    print(\"--- Researcher Agent Running ---\")\n",
    "    # In a real scenario, this would come from a web search or DB lookup\n",
    "    research_data = \"The Model Context Protocol (MCP) is a proposed standard for agent interoperability.\"\n",
    "    \n",
    "    print(\"  -> Found data, updating context server...\")\n",
    "    response = requests.post(\n",
    "        f\"{SERVER_URL}/context\", \n",
    "        json={\"key\": \"research_summary\", \"value\": research_data}\n",
    "    )\n",
    "    print(f\"  -> Server response: {response.json()}\")\n",
    "\n",
    "def writer_agent():\n",
    "    print(\"\\n--- Writer Agent Running ---\")\n",
    "    print(\"  -> Reading data from context server...\")\n",
    "    response = requests.get(f\"{SERVER_URL}/context\")\n",
    "    context = response.json()\n",
    "    summary = context.get(\"research_summary\", \"No summary found.\")\n",
    "    \n",
    "    print(\"  -> Generating article based on context:\")\n",
    "    article = f\"**Blog Post Title: The Future of AI is Collaborative**\\n\\n{summary}\"\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Simulate the Workflow\n",
    "\n",
    "# Initial state of the server\n",
    "print(f\"Initial server context: {requests.get(f'{SERVER_URL}/context').json()}\")\n",
    "\n",
    "# Run the first agent\n",
    "researcher_agent()\n",
    "\n",
    "# Check the server state now\n",
    "print(f\"\\nServer context after researcher: {requests.get(f'{SERVER_URL}/context').json()}\")\n",
    "\n",
    "# Run the second agent\n",
    "writer_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Why This is a Big Deal\n",
    "\n",
    "Notice that the `researcher_agent` and `writer_agent` never called each other directly. They only interacted with the central context server. This **decoupling** is the key.\n",
    "\n",
    "With a standardized protocol like MCP, you could have agents written in different languages, using different frameworks (CrewAI, LangGraph, AutoGen), all seamlessly collaborating on a single task. This is the foundation for a truly open and modular ecosystem of AI agents.\n",
    "\n",
    "Congratulations on completing the most forward-thinking chapter in this course! In the final chapter, we'll undertake a **capstone project** to combine everything you've learned into one impressive, modular agentic system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
